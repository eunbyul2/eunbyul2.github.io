---
layout: post
title: "CKA 정리: Multi-Container Pod(Init/Sidecar), Elastic(Filebeat), HPA/VPA + Metrics + 트러블슈팅"
date: 2026-01-16 23:00:00 +0900
categories: [Kubernetes, CKA, Autoscaling]
tags: [kubernetes, cka, pod, multi-container, initContainers, sidecar, filebeat, elasticsearch, kibana, hpa, vpa, metrics-server, kubectl-top, autoscaling, statefulset, qos, in-place-resize]
published: false
---

## 1. Multi-Container Pod: 왜 존재하고, 무엇을 “공유”하나

멀티 컨테이너 파드(Multi-container Pod)는 “마이크로서비스처럼 **분리된 이미지/배포 단위**는 유지하면서도, 런타임에서는 **항상 1:1로 붙어 같이 떠야 하는 기능**을 같은 파드에 배치”하는 방식

핵심은 **‘같이 스케일’ + ‘같이 죽고 살아야’** 하는 두(또는 그 이상) 컨테이너를 묶는 것

### 1-1) 마이크로서비스 분해와 멀티 컨테이너 파드의 관계

- 마이크로서비스: 서비스들을 독립적으로 개발/배포 → 변경/확장 용이
- 그런데 어떤 기능은 “서로 다른 서비스”이지만 **항상 같이 붙어야** 함.
  - 예: 메인 앱 + 로깅 에이전트, 메인 앱 + 프록시(Envoy), 메인 앱 + 파일 수집기(Filebeat) 등
- 이걸 서비스로 분리해 **서로 다른 파드**로 만들면?
  - 통신을 위해 Service가 필요할 수 있고
  - 로그/파일 같은 로컬 자원을 공유하려면 PV/PVC 설계가 더 복잡해지고
  - 무엇보다 “메인 앱 1개당 에이전트 1개” 같은 1:1 결합을 강제하기가 까다로워짐.

멀티 컨테이너 파드로 묶으면 “메인 앱이 3개로 늘어나면, 보조 컨테이너도 자동으로 3개가 같이 늘어남”을 자연스럽게 만족

### 왜 굳이 한 파드에 여러 컨테이너를 넣나?

예를 들어 `main app`과 `web server`가 다음 조건을 만족할 때 멀티 컨테이너 파드가 유리

- 두 기능은 코드/이미지 관점에서 서로 다른 책임(관심사) → 합쳐서 하나의 이미지로 만들면 비대해짐
- 그러나 런타임 관점에서는 “항상 같이 떠야 함” (one web per one app)
- 스케일링도 항상 같이 해야 함 (앱 인스턴스가 늘면 웹도 같이 늘어야 함)

### 1-2) 같은 파드에 있으면 자동으로 얻는 3가지 공유

멀티 컨테이너 파드의 컨테이너들은 다음을 공유

1) **Lifecycle 공유**
- 파드는 단일 오브젝트이므로, 파드 생성/삭제와 함께 컨테이너들이 함께 생성/종료
- “항상 같이 떠야 함”을 강제하는 가장 쉬운 방법.

2) **Network Namespace 공유**
- 같은 파드 내 컨테이너는 **같은 Pod IP**를 공유
- 그래서 컨테이너끼리 `localhost`로 통신 가능
  - 예: 앱이 `localhost:8080`을 열면, sidecar도 `localhost:8080`으로 접근 가능.

3) **Volume 공유**
- 같은 볼륨을 서로 다른 컨테이너에 마운트해서 **파일 기반으로 데이터 교환** 가능
  - 예: 앱이 `/log/app.log`에 파일 로그를 쓰고, sidecar가 같은 볼륨을 다른 mountPath로 읽어서 ES로 전송.

> 정리: 멀티 컨테이너 파드는 “파드 내부에서 공유로 해결”한다. 파드-파드 간 Service/볼륨 설계를 최소화

##2. 멀티 컨테이너 파드 패턴 3종: Co-located / Init / Sidecar

강의에서 말한 “멀티 컨테이너 파드 디자인 패턴”은 실제로는 **‘컨테이너가 언제 시작하고 언제 종료되는가’**에 따라 3종류로 분류하는 관점

### 2-1) Co-located containers (기본 멀티 컨테이너)

- `spec.containers` 배열에 컨테이너를 2개 이상 넣는 형태
- **모든 컨테이너가 파드 생애주기 내내 계속 실행**하는 전제
- 중요한 특징: **시작 순서를 보장하지 않는다**
  - “대체로 같이 뜬다”일 뿐, “A가 먼저 뜬다”는 보장 불가

예시:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: colocated-demo
spec:
  containers:
  - name: web
    image: nginx
  - name: app
    image: myapp:1.0
```

언제 쓰나?
- 서로 의존성이 있으나 “정확히 누가 먼저 뜨는지”가 중요하지 않을 때
- 둘 다 항상 상주해야 하는 컴포넌트일 때

### 2-2) InitContainers (초기화 전용)

InitContainer는 “메인 컨테이너 시작 전에 **한 번 실행하고 끝나는 작업**”을 위한 전용 컨테이너다.

- `spec.initContainers` 배열에 정의
- **순차 실행**
  - 1번 initContainer 성공 종료 → 2번 initContainer 시작 → … → 전부 완료 후 앱 컨테이너 시작
- initContainer는 “완료(exit 0)”가 의미 있는 컨테이너
- 실패하면 kubelet이 반복 재시작하며, 파드는 `Init:CrashLoopBackOff` 같은 상태로 보일 수 있음.

#### 2-2-1) 대표 유스케이스

- 외부 의존성 준비 대기 (DB, API, 메시지 브로커)
- Git clone, 바이너리 다운로드
- 템플릿 기반 설정 파일 생성
- 초기 마이그레이션/스키마 준비(단, 이건 운영에서는 Job/마이그레이션 전략과 함께 설계 필요)

예시(서비스 준비까지 nslookup로 대기):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-wait-demo
spec:
  containers:
  - name: app
    image: busybox:1.28
    command: ["sh","-c","echo app start; sleep 3600"]
  initContainers:
  - name: wait-myservice
    image: busybox:1.28
    command: ["sh","-c","until nslookup myservice; do echo waiting; sleep 2; done;"]
  - name: wait-mydb
    image: busybox:1.28
    command: ["sh","-c","until nslookup mydb; do echo waiting; sleep 2; done;"]
```

#### 2-2-2) initContainer 상태/로그 확인 루틴(시험/실무 공통)

1) 파드 상태 확인
```bash
kubectl get pod <pod>
```
- `Init:0/2`, `Init:1/2` 처럼 init 진행 상황이 표시될 수 있음.

2) 상세 확인
```bash
kubectl describe pod <pod>
```
- `Init Containers:` 섹션에서 `State`, `Reason`, `Exit Code`를 본다.
  - `Reason: Completed` + `Exit Code: 0` → 정상 완료
  - `Reason: Error` + `Exit Code: 127` → 보통 “커맨드 오타/바이너리 없음” 의심

3) initContainer 로그 확인(중요)
```bash
kubectl logs <pod> -c <init-container-name>
```

### 2-3) Sidecar container (보조 기능을 “항상 같이”)

Sidecar의 목적은 “메인 앱과 함께 살아있어야 하는 보조 기능”을 제공하는 것.

대표 사례:
- 로그 수집/전송(Filebeat, Fluent Bit)
- 프록시/서비스메시(Envoy)
- 보안 에이전트
- metrics exporter 등

#### 2-3-1) Co-located vs Sidecar가 헷갈리는 이유

둘 다 “파드 수명주기 내내 실행”될 수 있음.
그럼 왜 패턴을 나누나?

- Co-located(그냥 containers 배열 2개)는 **시작 순서 제어가 없다**
- Sidecar(강의에서 말한 ‘native sidecar’)는 **시작 순서를 의도적으로 만들 수 있다**

#### 2-3-2) Kubernetes native sidecar (initContainers + restartPolicy: Always)

핵심:
- sidecar를 `initContainers`에 정의
- `restartPolicy: Always`를 줘서 “완료되지 않고 계속 실행”되게 만듦.
- 이렇게 하면 “메인 앱이 시작되기 전에 sidecar가 먼저 올라가도록” 의도를 표현

예시(패턴만):

```yaml
initContainers:
- name: sidecar
  image: some/sidecar
  restartPolicy: Always
```

> 주의: 이건 “일반 initContainer(완료 후 종료)”와 다름. restartPolicy Always로 인해 종료하지 않고 계속 돌아가는 상태.

## 3. 실습: 멀티 컨테이너 파드(yellow) 생성과 트러블슈팅

요구사항:
- Pod: `yellow`
- Container1: `lemon` / `busybox`
- Container2: `gold` / `redis`
- CrashLoopBackOff면 lemon에 `sleep 1000` 추가

### 3-1) 왜 `kubectl run`으로 한 방에 안 되나?

`kubectl run`은 “단일 컨테이너 파드”를 빠르게 만드는 용도

자주 나오는 실수:
- `--image` 필수: `kubectl run yellow --dry-run=client -o yaml` → image 없어서 실패
- `--image=busybox,redis` → “쉼표로 이미지 나열”은 지원되는 문법이 아니다(이미지 이름 자체가 invalid format)
- `--name` 플래그 없음 → 파드 이름은 `kubectl run <NAME>` 위치 인자로 즘.

결론: 멀티 컨테이너 파드는 실전/시험에서 **YAML(선언형)로 만드는 게 정석**

### 3-2) busybox가 CrashLoop가 나기 쉬운 이유(중요)

busybox는 컨테이너가 “계속 살아있을 기본 프로세스”가 없으면 **바로 종료**
- 종료 → kubelet이 다시 띄움 → 반복 → CrashLoopBackOff

그래서 실습 조건이 “CrashLoopBackOff면 sleep 1000”인 것.
가장 안전한 방법은 애초에 넣는 것!

정답 YAML:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  containers:
  - name: lemon
    image: busybox
    command: ["sh","-c","sleep 1000"]
  - name: gold
    image: redis
```

### 3-3) strict decoding 에러: `namme` 오타

실제 에러:
```
strict decoding error: unknown field "spec.containers[1].namme"
```

의미:
- Kubernetes API 서버가 YAML을 JSON 스키마로 디코딩할 때
- “Pod spec에 namme라는 필드는 없다” → 거부

수정:
```yaml
- name: gold
  image: redis
```

## 4. Elastic Stack 실습: Kibana 확인 + 앱 로그 파일 확인 + Filebeat sidecar로 ES 전송

### 4-1) “Kibana UI 링크 들어가서 로그가 없을 것”의 뜻

- Kibana는 ES에 들어온 로그를 시각화하는 UI
- 아직 로그 수집/전송을 설정하지 않았으니 Kibana에서 검색해도 결과가 없다는 말
- Kibana Pod가 Ready여도 UI가 완전히 뜨는데 시간이 더 걸릴 수 있음(초기화/플러그인/연결)

Kibana 로그 확인:
```bash
kubectl -n elastic-stack logs kibana
```

### 4-2) “/log/app.log에서 로그인 문제 사용자 찾기”의 정확한 의미

앱이 stdout이 아니라 “파일”에 로그를 씀.

1) 어떤 파드가 앱인지 확인
```bash
kubectl -n elastic-stack get pods
```

2) 파드 안에 들어가 파일 확인
```bash
kubectl -n elastic-stack exec -it <app-pod> -- sh -c "tail -n 200 /log/app.log"
```

로그에서 `Login failed`, `Invalid password`, 특정 username 등을 찾아 “문제 user”를 파악

### 4-3) 실습 요구: Filebeat sidecar 붙이기

1) “기존 app pod에 sidecar 컨테이너 추가”
- Pod spec 변경은 기존 파드에 patch로 불가(컨테이너 추가는 불가) → **삭제 후 재생성** 필요

2) sidecar 정보
- name: `sidecar`
- image: `kodekloud/filebeat-configured`

3) 볼륨 마운트 조건
- 기존 `log-volume`을 sidecar에도 마운트
- sidecar mountPath는 `/var/log/event-simulator/`

4) “Kubernetes native sidecar” 방식
- **initContainers에 정의**
- **restartPolicy: Always**
- 계속 실행되며 main container와 함께 살아야 함.

예시(핵심만):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
spec:
  containers:
  - name: app
    image: <기존 앱 이미지>
    volumeMounts:
    - name: log-volume
      mountPath: /log
  initContainers:
  - name: sidecar
    image: kodekloud/filebeat-configured
    restartPolicy: Always
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/event-simulator/
  volumes:
  - name: log-volume
    emptyDir: {}
```

> 과제 주의사항: “기존 app 컨테이너/볼륨 정의 수정 금지”이므로, 볼륨 이름/타입을 바꾸면 감점/오답
**sidecar 쪽에만 mount를 추가**하는 관점으로 접근

## 5. InitContainer 문제풀이 방식(강의 스크립트 기반 정리)

### 5-1) initContainer가 있는 파드 찾는 법

`kubectl get pods`의 READY 컬럼은 “app containers만 카운트”
initContainer는 READY에 포함되지 않음.

정답 루트:
```bash
kubectl describe pod <pod>
```
출력에서 `Init Containers:` 섹션이 있으면 그 파드가 initContainer를 가진 것

### 5-2) initContainer 이미지/상태/종료 사유 확인

`kubectl describe pod blue` 같은 출력에서:

- Init Containers:
  - Name: init-service
  - Image: busybox
  - State: Terminated
  - Reason: Completed
  - Exit Code: 0

해석:
- initContainer가 정상 완료해서 종료되었고(Completed)
- exit code 0 → 성공
- 이후 app containers가 시작된 것

### 5-3) Init:CrashLoopBackOff 디버깅 루틴(

상태가 `Init:CrashLoopBackOff`면 대부분 initContainer가 실패하고 있다는 뜻.

1) initContainer 로그 보기
```bash
kubectl logs orange -c init-myservice
```

2) `sleep not found`면 거의 확정으로 커맨드 오타/실행파일 부재.
- busybox에는 `sleep`이 있으므로 “스펠링 오타”가 흔하다(예: `sleeeep`)

3) 수정 방법
- Pod는 spec 일부만 변경 가능 범위가 제한됨 → 보통 “edit → replace --force” 또는 재생성

## 6. Self Healing: ReplicaSet/ReplicationController가 말하는 ‘자동 복구’의 정체

강의 문구:
- “ReplicaSets/ReplicationControllers로 self-healing”
- “Pod가 죽으면 자동으로 재생성”
- “항상 지정한 replica 수를 유지”

정확히는:
- Deployment/ReplicaSet이 “원하는 상태(Desired: replicas=N)”를 유지하려 함.
- 어떤 파드가 크래시/노드 장애로 사라지면:
  - 컨트롤러가 현재 Running 수를 관찰하고
  - 부족하면 새 파드를 만들어 채운다.

여기서 흔한 오해:
- “앱 프로세스가 죽었는데 파드가 안 죽으면?”  
  → 그건 liveness/readiness probe가 필요할 수 있음.(강의는 CKA 범위에서 깊게 안 다룸)
- CKA에서는 “ReplicaSet이 파드 수를 유지한다” 관점이 중요

## 7. Scaling 개념 전체 정리: 수평/수직 + 워크로드/클러스터

오토스케일러를 이해하려면, 스케일링 축이 2개 필요

### 7-1) 스케일링 2축: 무엇을 늘리나?

1) **워크로드 스케일링**
- 파드 수(수평) 또는 파드 자원(수직)

2) **클러스터 인프라 스케일링**
- 노드 수(수평) 또는 노드 스펙(수직)

### 7-2) 수평 vs 수직

- 수직(Vertical): “한 개체의 크기(CPU/Memory)를 키움”
- 수평(Horizontal): “개체 수를 늘려 분산”

Kubernetes로 옮기면:
- 워크로드 수평: Pod replicas 증가
- 워크로드 수직: Pod resource requests/limits 증가
- 클러스터 수평: Node 추가
- 클러스터 수직: Node 스펙 상향(보통 새 노드로 교체)

### 7-3) 수동 vs 자동

- 수동 워크로드 수평: `kubectl scale`
- 수동 워크로드 수직: `kubectl edit deployment`로 requests/limits 조정(기본은 재생성)
- 자동 워크로드 수평: HPA
- 자동 워크로드 수직: VPA(별도 설치 필요)
- 자동 클러스터 수평: Cluster Autoscaler(노드 오토스케일)

## 8. Metrics / kubectl top / metrics-server가 없으면 왜 안 되나

###8-1) `kubectl top pod`가 의미하는 것

`kubectl top`은 노드/파드의 CPU/메모리 “현재 사용량”을 보여줌.
이 데이터는 kubelet이 직접 제공하는 게 아니라, 보통 **metrics-server**가 수집/집계해서 제공

그래서:
- metrics-server가 없으면 `kubectl top`이 실패하거나 값이 안 나옴.
- 새로 배포된 파드는 metrics가 “조금 늦게” 잡힐 수 있음.

### 8-2) “450m 임계값 근처”라는 말의 의미

- `m`은 millicore(1 core = 1000m)
- 예: 450m ≈ 0.45 CPU 코어 사용
- 운영자가 “이 정도면 증설해야 한다”라는 기준선을 정해두고
- top으로 관찰하다가 넘으면 스케일링을 결정한다는 뜻

## 9. HPA: kubectl autoscale 옵션, 동작 원리, 상태 확인

###9-1) `kubectl autoscale`가 하는 일

예시:
```bash
kubectl autoscale deployment myapp --cpu-percent=90 --min=1 --max=10
```

이 명령은 **HPA 리소스를 생성**
HPA는 Deployment의 replicas를 자동으로 조절

- `--cpu-percent=90`
  - 목표 CPU 사용률(%)
- `--min=1`
  - replicas 최소값
- `--max=10`
  - replicas 최대값

> HPA는 기본 내장 컴포넌트라 “따로 설치”가 아니라 “리소스 생성”이 핵심.
단, metrics-server는 필요

### 9-2) 상태 확인: get/describe에서 무엇을 보나

```bash
kubectl get hpa -n <ns>
kubectl describe hpa <hpa> -n <ns>
```

- get에서 `TARGETS (현재/목표)`, MIN/MAX, REPLICAS 확인
- describe에서 Metrics 소스/Events로 스케일 조건 확인

### 9-3) 수동 스케일 질문(함정): 리소스가 부족하면 무슨 일이?

`kubectl scale`로 replicas를 올렸는데 노드 리소스가 부족하면:
- 스케줄링 가능한 만큼은 Running
- 나머지는 Pending(스케줄러가 배치 못함)

정답 방향: “일부는 뜨지만 남은 건 Pending”

## 10. In-place resizing(인플레이스 리사이징) + VPA와의 관계

### 10-1) in-place = “그 자리에서”

파드를 죽이지 않고, 실행 중에 CPU/메모리 리소스를 조정하는 방향.

### 10-2) 기본 동작: 리소스 변경은 보통 파드 재생성

Deployment 템플릿 resources 변경 → 기존 파드 종료 → 새 파드 생성

### 10-3) in-place resizing이 되면 VPA Auto가 의미가 커짐

- 지금은 VPA가 추천 적용하려면 evict/재생성이 필요
- in-place가 안정화되면 “재생성 대신 리소스만 변경”이 가능해져
  - stateful workload에도 적용성이 좋아진다.

## 11. QoS Class(Guaranteed/Burstable/BestEffort) 요약

- Guaranteed: request=limit(모든 컨테이너)
- Burstable: request는 있으나 limit이 다르거나 일부만 설정
- BestEffort: request/limit 없음

자원 압박 시 eviction 우선순위에 영향.

## 12. StatefulSet 요약 + StatefulSet + VPA 포인트

StatefulSet:
- 안정적인 파드 이름(app-0, app-1)
- 순차 생성/삭제 경향
- PVC와 결합해 상태 저장

StatefulSet + VPA:
- 재시작/evict 민감 → updateMode Off/Initial로 먼저 추천만 보는 전략이 자주 안전
- Recreate/Auto는 운영 정책이 필요

## 13. VPA 설치/구성: CRD, RBAC, 구성요소 3개

- VPA는 기본 내장이 아니라 설치 필요(환경에 따라)
- CRD: VerticalPodAutoscaler 같은 새 kind를 API에 등록
- RBAC: VPA 컴포넌트가 필요한 권한을 갖게 함

컴포넌트:
- recommender: 추천 산출
- updater: 필요 시 evict 시도
- admission-controller: 생성 시점에 추천값을 pod spec에 반영

## 14. VPA Updater “too few replicas” 해석

replicas=1이면 마지막 파드 evict를 막는 안전장치 때문에 VPA 적용이 막힐 수 있음.
해결: replicas를 2 이상으로.

## 15. VPA CPU Optimization Lab: top 변동 + 추천값 저장

- top 값은 스냅샷이라 변동 정상
- describe vpa의 Target이 추천값
- /root/target에 echo로 저장하거나 awk로 추출

## 16. VPA + HPA 같이 쓰면 안 되는 이유

VPA가 requests를 바꾸면, HPA의 “사용률” 계산 기준이 흔들릴 수 있어 스케일이 출렁일 수 있음.
따라서 기본적으로는 같이 쓰지 않는 게 안전하고, 같이 쓸 때는 metric/정책 설계가 필요

## 17. `autoscaling.k8s.io` 같은 문자열의 정체(API Group)

`apps/v1`, `autoscaling.k8s.io/v1` 같은 것은 API Group/Version.
kind가 어느 그룹/버전의 리소스인지 식별하는 “네임스페이스”