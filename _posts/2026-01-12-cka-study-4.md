---
layout: post
title: "CKA 정리: Scheduling, Resource, Static Pod, PriorityClass"
date: 2026-01-12 22:50:00 +0900
categories: [Kubernetes, CKA]
tags: [kubernetes, cka, scheduling, static-pod, daemonset, priorityclass, resources]
published: true
---

## 1. Kubernetes 스케줄링 기본 개념
### 1-1) Scheduler의 역할
- Pod를 어떤 Node에 배치할지 결정\
  - 스케줄링 판단에 주로 쓰는 정보:
  - Pod의 **resource requests**
  - Node의 **남은 자원(allocatable - allocated)**
  - **taints/tolerations**, **node affinity**, **nodeSelector**, **(pod) affinity/anti-affinity** 등 제약조건
- CPU/Memory 요청(requests), 제약조건을 기준으로 판단
- 조건을 만족하는 Node가 없으면 Pod는 Pending 상태가 되어 `Events`에 원인이 찍힘.

```bash
kubectl describe pod <pod-name> | sed -n '/Events/,$p'
```

### 1-2) Node Selector와 Node Affinity
- Node Selector: 단순 key=value 매칭
- Node Affinity: AND/OR/NOT 같은 복잡한 조건 가능
| 항목 | NodeSelector | NodeAffinity |
|---|---|---|
| 표현력 | key=value 1차원 | In/NotIn/Exists/DoesNotExist, OR(terms) 등 |
| 난이도 | 쉬움 | 더 복잡 |
| 자주 쓰는 목적 | 특정 라벨 노드 지정 | “조건 조합”으로 노드 필터링 |

- NodeSelector 예시 (단순 매칭):
```yaml
spec:
  nodeSelector:
    color: blue
```

- Node Affinity 예시 (**강제 조건**): `requiredDuringSchedulingIgnoredDuringExecution`
```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values: ["blue"]
```

> `requiredDuringSchedulingIgnoredDuringExecution` 의미  
> - **Scheduling 시점에 반드시 만족**(못 맞추면 Pending)  
> - 실행 중에는(ignoredDuringExecution) 노드 라벨이 바뀌어도 Pod를 “즉시 쫓아내진” 않음.

- `operator` 종류(시험에 자주 나옴)

| operator | values 필요? | 의미 |
|---|---:|---|
| In | O | key의 값이 values 중 하나 |
| NotIn | O | key의 값이 values에 없음 |
| Exists | X | key 라벨이 존재하면 OK |
| DoesNotExist | X | key 라벨이 없어야 OK |
| Gt / Lt | O(숫자 1개) | 값 비교(드물지만 나옴) |

### 1-3) Taints & Tolerations
- Node가 Pod를 거부하도록 설정
- **Taint**(노드): “이 조건 못 맞추는 Pod는 오지 마”
- **Toleration**(파드): “나는 그 Taint를 견딜 수 있어”
- 단독 사용 시 '배치 보장'은 되지만 '전용화'는 불완전

중요:
- Taint/Toleration은 **‘막는 역할’**이 강함  
- “내 Pod가 특정 노드로 ‘가야만 한다’”를 보장하지는 못함(=그 노드로 **선호/강제 배치**는 NodeAffinity/Selector)

### 1-4) Node Affinity + Taints 조합
- Taints/Tolerations: 다른 Pod 유입 차단
- Node Affinity: 내 Pod가 지정 Node로만 가도록 제한
- 두 개를 함께 써야 Node 전용화 가능
- 대표 조합(전용 노드 만들기):
- **taint**로 *다른 팀 Pod 유입을 막고*
- **nodeAffinity**로 *내 Pod를 그 노드로만 가게 제한*

### 1-5) 연습문제 예시: “Which nodes can the pods for the blue deployment be placed on?”
의미:
- blue deployment의 Pod가 **어떤 노드에 스케줄 가능한지** 확인하라
- **두 노드 모두의 taint 여부**를 체크하라 (toleration 없으면 NoSchedule taint 노드는 불가)

실전 체크 순서:
```bash
# 1) 노드 라벨/taint 확인
kubectl get nodes --show-labels
kubectl describe node node01 | sed -n '/Taints/,$p'
kubectl describe node controlplane | sed -n '/Taints/,$p'

# 2) blue deployment의 pod spec 확인(affinity/tolerations/nodeSelector 등)
kubectl get deploy blue -o yaml | sed -n '/template:/,$p'
```

### 1-6) “node에 scale 하려다 실패”한 이유
실제 실수:
```bash
kubectl scale node node01 -l color=blue
# error: required flag(s) "replicas" not set
```

정리:
- `kubectl scale`은 **Deployment/ReplicaSet/StatefulSet** 같은 “replicas 가진 리소스”에 쓰는 명령
- **Node는 scale 대상이 아님**
- 그래서 `--replicas`를 요구하는 에러가 뜬다

노드에 뭔가 “적용”하려면 보통 라벨/taint다:
```bash
kubectl label node node01 color=blue
kubectl taint node node01 color=blue:NoSchedule
```

## 2. Resource Requests & Limits
### 2-1) CPU와 Memory 요청(Requests)
- Scheduler가 Pod 배치 시 참고하는 최소 요구량
- CPU 단위: vCPU, millicore (500m = 0.5 CPU)
- Memory 단위: Mi(Mebibyte), Gi(Gibibyte)

| 항목 | Requests | Limits |
|---|---|---|
| 의미 | “최소 보장/스케줄 기준” | “최대 사용 상한” |
| 스케줄러가 사용? | O | (직접 기준은 request) |
| 초과 시 CPU | (없음) | **throttle(느려짐)** |
| 초과 시 Memory | (없음) | **OOMKilled(종료)** |

| 표기 | 의미 | 기준 |
|---|---|---|
| **Mi** | Mebibyte | 1024^2 bytes |
| **Gi** | Gibibyte | 1024^3 bytes |
| **M, G** | (SI) Mega/Giga | 1000 기반 |

시험에서는 보통 `Mi/Gi`를 많이 쓴다.

### 2-2) CPU vs vCPU vs Memory 단위
- vCPU: 논리 CPU (클라우드 기준)
- Mi: 1024^2 Byte
- CPU는 throttle, Memory는 OOM Kill

- 쿠버네티스 YAML에 쓰는 `cpu: 1`은 “CPU 1코어” 개념으로 쓰인다.
- 클라우드 문서에서 흔히 말하는 **vCPU**는 “논리 CPU 단위”를 뜻함.
- 쿠버네티스에서 `1 CPU`는 보통 환경에서 “1 vCPU(또는 1 core에 준하는 스케줄 단위)”로 이해하면 된다.

> 정확한 물리 매핑은 하드웨어/가상화에 따라 달라질 수 있음(환경 의존).  
> 하지만 시험/실무 운영 관점에서 **‘스케줄 단위’로 1CPU**라고 이해하면 충분.

### 2-3) Limits의 의미
- CPU limit 초과: 느려질 뿐
- Memory limit 초과: Pod 강제 종료(OOMKilled)

### 2-4) 이상적인 설정 전략
- CPU: requests만 설정, limits 생략
- Memory: requests + limits 둘 다 설정

### “파드 하나가 꽉 차면 다른 파드가 죽나?”
정확히는 이렇게 이해하면 된다:

- **CPU**: 초과하려고 하면 throttle → 다른 파드가 “바로 죽는” 게 아니라 전반적으로 느려짐
- **Memory**: limit을 넘겨서 계속 쓰면 **그 컨테이너/파드가 OOMKilled** 될 수 있음  
  - “다른 파드가 죽는다”라기보단, 보통은 **문제 일으킨 파드가 죽음**
  - 단, 노드 전체 메모리 압박이 심하면 시스템이 다른 파드를 희생시키는 상황도 가능(노드 메모리 압박/eviction 시나리오)

시험 대응:
```bash
kubectl describe pod <pod> | egrep -i 'oom|killed|evict|memory|events' -A5
```

### OOM 났는데 `kubectl edit pod`가 안 됐던 이유(핵심)
Pod의 많은 spec 필드는 **immutable(변경 불가)**
특히 “컨테이너 리소스” 같은 변경을 하려다 edit이 실패하고 이런 메시지를 보게 됨:
- `pods "<name>" is invalid ...`
- `A copy of your changes has been stored to "/tmp/kubectl-edit-xxxx.yaml"`

해결 패턴(시험에서 안전):
1) Pod YAML을 수정(또는 임시파일 수정)
2) 기존 Pod 삭제
3) 수정된 YAML로 재생성

```bash
kubectl delete pod elephant
kubectl apply -f /tmp/kubectl-edit-xxxx.yaml
```

### 2-5) LimitRange와 ResourceQuota
- LimitRange: Pod 기본값 강제 (namespace 단위)
- ResourceQuota: namespace 전체 자원 상한

## 3. DaemonSet (한 노드당 1개 Pod)
### 3-1) DaemonSet이란?
- 모든 Node에 1개씩 Pod 보장
- Node 추가/삭제 시 자동 반영

### 3-2) 주요 사용 사례
- kube-proxy
- CNI (flannel, calico)
- 로그 수집, 모니터링 에이전트

### 3-3) ReplicaSet과 차이
- ReplicaSet: 개수 기준
- DaemonSet: Node 기준

### “Which of the below is a DaemonSet?” 문제 해석
의미:
- 주어진 리소스 중에 “각 노드마다 하나씩 뜨는 것”이 무엇인지 묻는 문제
- 실전에서는 `kubectl get ds -A`로 확인

```bash
kubectl get daemonset -A
kubectl get ds -A
```

### `kubectl create daemonset ... --image` 에러 원인과 정답 커맨드
내가 겪은 에러:
```bash
kubectl create daemonset elasticsearch -n kube-system --image=... --dry-run=client
error: unknown flag: --image
```

정리:
- `kubectl create daemonset`은 버전/환경에 따라 `--image` 플래그가 없을 수 있음
- 시험에서는 가장 안전하게 **YAML로 생성**하거나,
- 또는 `kubectl create deployment`/`kubectl run`처럼 플래그가 확실한 명령을 쓰되, DS는 YAML이 안정적

안전한 DS YAML 예시:
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: registry.k8s.io/fluentd-elasticsearch:1.20
```

```bash
kubectl apply -f ds.yaml
```

## 4. Static Pod
### 4-1) Static Pod 개념
- kubelet이 직접 생성
- API Server 없이도 동작
- 로컬 manifest 파일 기반

| 구분 | 일반 Pod | Static Pod |
|---|---|---|
| 생성 주체 | API Server/Controller + kubelet | **kubelet 단독** |
| 정의 위치 | etcd(클러스터 상태) | **노드 로컬 파일** |
| kubectl delete | 삭제 가능(컨트롤러에 따라 재생성) | “지워도 다시 생김” |
| 수정/삭제 방법 | kubectl apply/edit/delete | **manifest 파일 수정/삭제** |

### 4-2) Static Pod 경로
Static Pod 경로 확인(시험 루트)
1) kubelet config에서 확인:
```bash
cat /var/lib/kubelet/config.yaml | grep -i staticPodPath
```

2) 없으면 kubelet 실행 옵션 확인:
```bash
ps -ef | grep kubelet
```

대부분 kubeadm 환경이면:
```text
/etc/kubernetes/manifests
```

- kubelet 설정의 `staticPodPath`
- 일반적으로 `/etc/kubernetes/manifests`

### 4-3) Static Pod 확인 방법
- `kubectl get pods -A` (mirror pod)
- 이름에 Node 이름 포함
- annotation: `kubernetes.io/config.source: file`

### “Static Pod 개수” 문제ㅁ
`kubectl get pods -A`에서 Static Pod는 보통 컨트롤플레인 4종:
- `etcd-<node>`
- `kube-apiserver-<node>`
- `kube-controller-manager-<node>`
- `kube-scheduler-<node>`

DaemonSet과 헷갈리면 오답:
- `kube-proxy-*`는 **DaemonSet**
- `kube-flannel-ds-*`도 **DaemonSet**
- `coredns-*`는 보통 **Deployment**

Static Pod 확정 체크(Annotation):
```bash
kubectl describe pod kube-apiserver-controlplane -n kube-system | grep -i config.source -A2
# kubernetes.io/config.source: file
```

### 4-4) Static Pod 생성/수정/삭제
- 생성: manifest 디렉터리에 YAML 추가
- 수정: 파일 수정
- 삭제: 파일 삭제 (kubectl delete로는 불가)

### “삭제해도 또 살아남”의 정확한 이유와 해결
현상:
- `kubectl delete pod <static-pod>` → 삭제 메시지는 뜨는데 곧 다시 생김

이유:
- API Server에 보이는 건 **mirror pod**
- 원본은 **노드의 manifest 파일**
- kubelet이 파일을 계속 읽어서 **재생성**

정답 루트:
```bash
# 1) manifest 디렉터리 확인
ls /etc/kubernetes/manifests

# 2) 해당 YAML 삭제(또는 이름 변경)
sudo rm /etc/kubernetes/manifests/<pod>.yaml
# 또는 mv로 치워두기
sudo mv /etc/kubernetes/manifests/<pod>.yaml /root/

# 3) kubelet이 반영하면 pod가 사라짐
kubectl get pods -A | grep <pod-name>
```

### 4-5) Static Pod vs DaemonSet
- Static Pod: kubelet 단독
- DaemonSet: API Server + Controller

## 5. PriorityClass
### 5-1) PriorityClass 개념
- Pod 중요도 정의
- PriorityClass는 **클러스터 전역(Non-namespaced)** 리소스
- 숫자가 클수록 우선순위 높음
- 기본값: priorityClassName 없으면 **priority=0**
- cluster-scoped 리소스

```bash
kubectl get priorityclass
```

### 5-2) 기본 PriorityClass
- system-cluster-critical
- system-node-critical

### 5-3) Pod에 PriorityClass 적용
```yaml
spec:
  priorityClassName: high-priority
```

### 5-4) Preemption (선점)
- 기본: 낮은 priority Pod 제거
- `preemptionPolicy: Never` 설정 시 제거 안 함

### 5-5) Priority 수정 전략
- Pod는 priority 수정 불가
- 삭제 후 재생성 필수

### “파일 수정이 안 돼서 /tmp 임시파일”이 생겼을 때
`kubectl edit` 실패 시:
- kubectl이 변경 내용을 `/tmp/kubectl-edit-xxxx.yaml`에 저장해 둠

실전 해결:
1) 그 임시파일을 열어 `spec.priorityClassName`을 넣고
2) `status:` 삭제 + metadata 불필요 필드 제거(있으면)
3) 삭제 후 재생성

```bash
vi /tmp/kubectl-edit-xxxx.yaml
kubectl delete pod critical-app
kubectl apply -f /tmp/kubectl-edit-xxxx.yaml
```

### 5-6) PreemptionPolicy(선점) 요약
| 설정 | 의미 | 결과 |
|---|---|---|
| (기본) PreemptLowerPriority | 낮은 우선순위 Pod를 **죽여서 자리 확보** | 중요 Pod가 뜨기 쉬움 |
| Never | 다른 Pod를 죽이지 않음 | 리소스 생길 때까지 대기 |

## 6. CKA 시험 실전 포인트

### 6-1) Static Pod 문제 풀이 루트
1. `kubectl get pods -A`
2. control-plane Pod 식별
3. `/etc/kubernetes/manifests` 확인

- “몇 개냐?” → `ls /etc/kubernetes/manifests | wc -l`
- “경로가 뭐냐?” → `staticPodPath` 확인
- “이미지 뭐 쓰냐?” → `/etc/kubernetes/manifests/kube-apiserver.yaml`에서 `image:` grep

```bash
grep -n "image:" /etc/kubernetes/manifests/kube-apiserver.yaml
```

### 6-2) Priority 문제 풀이 루트
1. PriorityClass 존재 확인
2. Pod YAML에 연결
3. 삭제 후 재생성
4. Running 상태 확인

### 6-3) 자주 나오는 함정
- DaemonSet을 Static Pod로 착각
- kubectl delete로 Static Pod 삭제 시도
- Priority 수정 edit 시도

### DaemonSet/Static Pod 구분 루트
| 대상 | 확인 명령 | 특징 |
|---|---|---|
| Static Pod | describe에서 `config.source: file` | kubelet + 파일 |
| DaemonSet Pod | `Controlled By: DaemonSet/...` | 노드당 1개 |
| Deployment Pod | `Controlled By: ReplicaSet/...` | replicas 기반 |

### Resources 문제 루트
1) Pending이면 `Events` 확인
2) request/limit 확인
3) 노드 allocatable 확인

```bash
kubectl describe pod <pod> | egrep -i "Requests|Limits|Events" -A5
kubectl describe node <node> | egrep -i "Allocatable|Allocated resources" -A20
```

## 7. 전체 요약
- 스케줄링은 조건 + 리소스 + 우선순위의 조합
- **Taint/Toleration은 “막는 것”**, **Affinity는 “가게 하는 것”**
- CPU limit 초과는 throttle, Memory limit 초과는 OOMKilled가 핵심
- DaemonSet은 “노드마다 1개”, Static Pod는 “kubelet이 파일로 생성”이다.
- Static Pod는 kubelet과 파일이 전부
- PriorityClass는 “리소스 부족 시 누가 살아남는가”를 결정 즉, 위기 상황에서 살아남는 기준