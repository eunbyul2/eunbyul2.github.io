---
layout: post
title: "CKA 정리: 멀티 스케줄러/스케줄러 프로파일 + Admission Controller/Webhook"
date: 2026-01-13 23:00:00 +0900
categories: [Kubernetes, CKA]
tags: [kubernetes, cka, scheduler, kube-scheduler, schedulerName, leaderElection, HA, profiles, plugins, admission, webhook, kubeadm, static-pod, kube-apiserver, tls, configmap, secrets]
published: true
---
## 1. 전체 큰 흐름: “요청이 들어오면” Kubernetes는 어디를 지나가나

### 1-1) 컨트롤플레인 요청 처리 파이프라인(핵심)
사용자가 `kubectl`로 요청을 보내면(예: Pod 생성):

1) **kubectl → kube-apiserver** (REST API 요청)  
2) kube-apiserver 내부에서 순서대로:
   - **Authentication(인증)**: “누구냐?” (증명서/토큰 등으로 사용자 식별)
   - **Authorization(인가)**: “이 작업 할 권한 있냐?” (RBAC: Role/ClusterRole + Binding)
   - **Admission(어드미션)**: “요청 내용 자체가 정책을 만족하냐? 수정/거부할까?”  
     - 여기서 **Admission Controllers / Webhooks**가 동작
3) 통과하면 오브젝트가 **etcd에 저장**  
4) 이후 컨트롤러/스케줄러/kubelet 등이 관여하며 실제 상태로 수렴

> 정리: **인증/인가가 “누가/권한”이라면, Admission은 “내용(정책)”**

## 2. 멀티 스케줄러: 기본 스케줄러 말고 “추가 스케줄러”를 왜/어떻게 쓰나

### 2-1) kube-scheduler의 역할
**Pending 상태의 Pod를 어떤 Node에 배치할지 결정**하고, 그 결과를 API 서버에 “바인딩”

### 2-2) 왜 커스텀/추가 스케줄러가 필요한가
기본 스케줄러는 **일반적인 제약(taints/tolerations, nodeAffinity, 리소스 등)**을 잘 처리하지만,
- “특정 앱은 배치 전 추가 검증이 필요”  
- “사내 정책/특수 하드웨어/특정 노드군 스케줄링 로직”  
같은 요구가 있으면 **스케줄링 알고리즘 자체를 바꾸거나 확장**하고 싶어짐.

이때 선택지:
- **(A) 별도의 스케줄러 프로세스/Pod를 추가로 띄우기 (멀티 스케줄러)**
- **(B) 하나의 kube-scheduler 바이너리 안에서 프로파일로 분리 (스케줄러 프로파일)**

## 3. kubeadm 환경, 스태틱 파드, 그리고 “왜 kube-scheduler Pod 이름을 찾는 문제가 나오는가”

### 3-1) kubeadm 기반 컨트롤플레인 특징(시험/실습에서 매우 자주 등장)
kubeadm 클러스터에서는 보통 컨트롤플레인 컴포넌트가:
- `kube-apiserver`
- `kube-controller-manager`
- `kube-scheduler`
- `etcd`
등이 **kube-system 네임스페이스의 Pod로 실행**

그리고 이들은 대개 **스태틱 파드(static pod)** 로 동작

### 3-2) 스태틱 파드(static pod)란?
- kubelet이 **로컬 파일(매니페스트)** 을 감시하다가,
- 파일이 있으면 Pod로 띄우는 방식
- 일반 Deployment처럼 API 서버가 스케줄링해서 만든 Pod가 아니라,
  **노드(컨트롤플레인)의 kubelet이 로컬 파일 기반으로 직접 띄움**

그래서 파일 경로가 중요해진다.

### 3-3) 스태틱 파드 매니페스트 기본 경로
대부분 kubeadm에서는:
- `/etc/kubernetes/manifests/`

여기에 `kube-apiserver.yaml`, `kube-scheduler.yaml` 같은 파일이 있고,
kubelet이 그 파일을 읽어 컨테이너를 띄운다.

### 3-4) “그 경로를 어떻게 찾아내는가?” (헷갈렸던 포인트)
정석은 kubelet의 설정에서 “staticPodPath”를 찾는 것

1) kubelet 실행 옵션에서 config 파일 위치 확인:
```bash
ps -ef | grep kubelet
# 예: --config=/var/lib/kubelet/config.yaml
```

2) kubelet config에서 static pod path 확인:
```bash
cat /var/lib/kubelet/config.yaml | grep -i staticpod
# 보통: staticPodPath: /etc/kubernetes/manifests
```

> 네가 말한 “`cat /var/bin/kubelet... | grep -i path`”는 상황에 따라 **kubelet config 파일 경로를 찾아서 그 안에서 staticPodPath를 찾는 과정**으로 이해

### 3-5) 그런데 “kubectl get pods”에서 아무것도 안 뜨는 문제는?
문제 문맥에 따라 다르지만, 대표 원인은:
- **네임스페이스를 안 봄**: 컨트롤플레인은 보통 `kube-system`
  ```bash
  kubectl get pods -n kube-system
  ```
- **리소스 이름을 잘못 쿼리**: 예) `kube-apiserver`라는 이름의 Pod가 없고
  실제는 `kube-apiserver-controlplane` 같은 형태일 수 있음.
- **클러스터 접근 자체가 안 됨**: kubeconfig 컨텍스트/권한/네트워크 문제

> 결론: 컨트롤플레인 Pod 찾는 문제는 거의 항상 `-n kube-system`부터 확인한다.

## 4. 멀티 스케줄러 실습: “추가 스케줄러 배포” 문제 해석 & 핵심 포인트

### 4-1) 문제 문장 해석(대표)
- “기본 kube-scheduler와 **같은 이미지**를 사용해 추가 스케줄러를 배포하라”
- `/root/my-scheduler.yaml` 제공
- 기본 scheduler 이미지는 다음으로 확인:
  ```bash
  kubectl describe pod kube-scheduler-controlplane -n kube-system
  ```

의도:
- 기본 스케줄러 Pod의 이미지(예: `registry.k8s.io/kube-scheduler:vX.Y.Z`)를 확인하고
- 동일한 이미지를 사용해 `my-scheduler`라는 별도 스케줄러를 띄우라는 뜻

### 4-2) Pod/Deployment에 “이 스케줄러로 배치해”라고 지정하는 필드
**Pod 스펙에는 반드시 `schedulerName`** 필드를 쓴다.

올바른 예:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  schedulerName: my-scheduler
  containers:
  - name: nginx
    image: nginx
```

## 5. “왜 추가 스케줄러는 config 파일 경로를 요구하나?” (ConfigMap/VolumeMount 핵심)

### 5-1) kube-scheduler는 설정을 어디서 읽나
kube-scheduler는 실행 시 보통 `--config=...` 로 설정 파일을 받음.
예:
```bash
/usr/local/bin/kube-scheduler --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
```

이때 `--config`가 **컨테이너 내부 파일 경로**를 요구하는 이유:
- kube-scheduler는 “YAML config 파일”을 파싱해서
  - schedulerName
  - leaderElection
  - plugins/profiles(프로파일)
  등을 결정하기 때문이다.
- 즉, 단순 환경변수로 끝나는 설정이 아니라 **구조화된 설정 파일**이 필요

### 5-2) 그 파일을 컨테이너 안에 어떻게 넣나?
대표 방식:
1) 이미지에 파일을 bake-in (비추: 변경 시 이미지 재빌드 필요)
2) **ConfigMap을 Volume으로 마운트 (권장)**
3) Secret을 Volume으로 마운트 (민감정보일 때)

이 실습은 2번을 사용했다.

### 5-3) “컨테이너 안에 파일처럼 마운트”하면 뭐가 좋은가?
- **이미지 변경 없이** 설정만 바꿀 수 있음
- 설정을 GitOps(ArgoCD 등)로 관리하기 쉬움
- 여러 환경(DEV/PROD)에서 값만 바꿔 재사용 가능
- 롤백/비교가 쉽다

### 5-4) `kubectl create configmap ... --from-file` 의미
예:
```bash
kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system
```

- `--from-file=/path/to/file`은
  - 해당 파일 내용을 ConfigMap `data`에 “키=파일명, 값=파일내용” 형태로 저장한다.
- 그리고 나중에 Pod에서:
  - `volumes: configMap: name: ...`
  - `volumeMounts: mountPath: ...`
  하면 **파일 형태로 컨테이너에 나타난다.**

## 6. HA(고가용성)와 leaderElection: “왜 필요한가”

### 6-1) HA(High Availability, 고가용성)란?
한 노드/한 프로세스가 죽어도 서비스(컨트롤플레인)가 계속 동작하도록
- **여러 대의 컨트롤플레인 노드**
- **중복 실행**
- **자동 리더 선출**
로 구성하는 것.

### 6-2) 스케줄러가 HA일 때 발생하는 문제
컨트롤플레인 노드가 2~3개이고,
각 노드에서 kube-scheduler가 동시에 실행되면:

- **동시에 같은 Pod를 스케줄링하려는 경쟁(race)** 가능성이 생김
- 따라서 “실제로 스케줄링 결정을 내리는 **활성(leader)** 는 하나만” 필요

### 6-3) leaderElection 옵션의 의미
- leaderElection이 켜져 있으면 여러 스케줄러 인스턴스 중 하나가 leader가 되고,
나머지는 standby가 됨.
- leader가 죽으면 다른 인스턴스가 leader를 승계

실습에서는 replicas=1이어서 보통 `leaderElect: false`로 단순화하기도 함.

## 7. 스케줄러 프로파일 & 플러그인: “멀티 스케줄러와 뭐가 다른가?”

### 7-1) 멀티 스케줄러(별도 Pod/프로세스) 방식의 문제
- 스케줄러가 여러 개면 **각자 독립적으로 판단**할 수 있어
  같은 노드 자원을 동시에 고려하며 경쟁할 수 있다(레이스).
- 운영/관리(로그, 배포, 업그레이드)가 복잡해짐.

### 7-2) 스케줄러 프로파일(Scheduling Profiles)
Kubernetes v1.18에서 “**하나의 kube-scheduler 바이너리** 안에
여러 프로파일(논리적 스케줄러)을 두는 방식”이 도입됨.

- 프로파일마다 `schedulerName`이 다르다.
- Pod의 `spec.schedulerName`으로 어떤 프로파일을 쓸지 선택
- 하지만 **프로세스는 하나**라서 운영이 단순하고 레이스 위험이 줄어듦.

### 7-3) 플러그인(plugin)이 정확히 무엇인가?
여기서 “플러그인”은:
- kube-scheduler 내부의 “확장 가능한 모듈”
- 스케줄링 단계(Queue/Filter/Score/Bind 등)마다 특정 로직을 수행

즉 “외부에서 설치하는 플러그인”이라기보다,
**스케줄러 프레임워크가 호출하는 스케줄링 로직 컴포넌트**

### 7-4) Filter / Score / Bind 헷갈릴 때 구분법
- **Filter = 탈락(불가능) 판정 단계**
  - “이 노드는 아예 안 됨” → 후보에서 제거
  - 예: 리소스 부족, nodeName 불일치, Unschedulable 노드 등
- **Score = 후보들 중 “점수 매기기”**
  - 남은 후보 노드들에 점수를 주고 최선의 노드를 선택
  - 예: 리소스 여유, 이미지 locality 등
  - Score는 보통 “가능/불가능”을 결정하지 않음(그건 Filter에서)
- **Bind = 최종 배치 확정**
  - 선택된 노드에 Pod를 바인딩(할당)하는 단계

한 문장:
- Filter: “되냐 안 되냐”
- Score: “되면 어디가 더 좋냐”
- Bind: “그럼 여기로 확정”

### 7-5) “어떤 요구사항이면 어느 단계에 플러그인을 넣나” 설계 예시
- 요구사항: “GPU가 반드시 있어야 한다” → GPU 없는 노드는 **불가능** → Filter
- 요구사항: “가능하면 이미지가 이미 캐시된 노드에 배치” → 캐시 없다고 불가능은 아님 → Score
- 요구사항: “배치 확정 전 외부 승인 필요” → Permit/PreBind 같은 확장 포인트 고려

## 8. Admission Controllers: RBAC로는 못 하는 “정책 기반 검증/수정”

### 8-1) RBAC(인가)와 Admission의 차이
- **RBAC(Authorization)**: “이 사용자가 Pod CREATE를 할 권한이 있는가?”
- **Admission**: “그 Pod spec이 우리 정책을 만족하는가?”
  - 이미지 레지스트리 제한, latest 금지, root 금지, 필수 라벨 강제, PVC StorageClass 자동 부여 등

### 8-2) Admission Controller는 옵션인가?
Admission Controller는 kube-apiserver의 **플러그인 목록**이며,
- `--enable-admission-plugins`
- `--disable-admission-plugins`
로 활성/비활성을 제어

### 8-3) Mutating vs Validating + 호출 순서
- **Mutating**: 요청 객체를 수정 가능 (예: DefaultStorageClass)
- **Validating**: 허용/거부만 (예: NamespaceExists/NamespaceLifecycle)
- 호출 순서: **Mutating → Validating**

## 9. kube-apiserver에서 admission plugins 확인/변경 루틴

### 9-1) Pod 이름을 정확히 확인
```bash
kubectl get pods -n kube-system
# kube-apiserver-controlplane 같은 이름
```

### 9-2) 기본 enabled 목록 확인(환경 기준)
```bash
kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins
```

### 9-3) 이 클러스터에서 “추가로 켠 것” 확인
```bash
grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml
```

## 10. Admission Controllers 문제 풀이(정답/이유)

### 10-1) What is not a function of admission controller?
정답: **authenticate user**  
이유: 인증(Authentication)은 Admission 전에 끝난다.

### 10-2) Mutating/Validating 조합 문제
정답: **(1) NamespaceAutoProvision-Mutating, NamespaceExists-Validating**  
이유:
- AutoProvision은 “없는 네임스페이스를 만들어 주는” **변경(뮤테이션)** 행위
- Exists는 “있는지 검사해 거부/허용”하는 **검증(밸리데이션)**

### 10-3) Admission controller invocation flow
정답: **(1) First Mutating then Validating**  
이유: 수정된 결과를 기준으로 검증해야 함

## 11. Admission Webhook: 외부 정책 서버 붙이기

### 11-1) Webhook이란?
요청 이벤트가 발생하면 kube-apiserver가 미리 등록된 서버로 콜백(HTTPS)을 보내서
허용/거부(Validating) 또는 수정(Mutating) 결정을 받는 방식.

### 11-2) MutatingAdmissionWebhook vs ValidatingAdmissionWebhook
- MutatingAdmissionWebhook: 외부 서버가 JSONPatch로 요청 객체를 수정
- ValidatingAdmissionWebhook: 외부 서버가 allowed=true/false로 허용/거부 결정

### 11-3) TLS가 왜 필요한가
kube-apiserver ↔ webhook 서버 통신은 **HTTPS(TLS)** 가 원칙
- 서버는 cert/key로 HTTPS를 열고
- apiserver는 caBundle로 서버 인증서가 신뢰 가능한지 검증

## 12. 실습 YAML 설명(Deployment/Service/WebhookConfiguration)

### 12-1) webhook-deployment.yaml 핵심
- `image: stackrox/admission-controller-webhook-demo:latest` : 데모 서버 이미지
- `ports: containerPort: 8443` : 서버가 HTTPS로 리슨하는 포트
- `volumeMounts` + `volumes.secret` :
  - TLS secret(`webhook-server-tls`)을 컨테이너에 파일로 마운트
  - `mountPath: /run/secrets/tls`에 cert/key 파일이 생김
- `securityContext.runAsNonRoot: true`, `runAsUser: 1234` :
  - 컨테이너 자체를 non-root로 실행(보안)

### 12-2) webhook-service.yaml 핵심
- Service 포트 `443` → targetPort `webhook-api(=8443)`
- selector `app: webhook-server`로 Deployment의 Pod를 연결

### 12-3) webhook-configuration.yaml에서 “무엇에 영향?”
rules:
- operations: CREATE
- apiGroups: [""]
- apiVersions: ["v1"]
- resources: ["pods"]

따라서 영향: **Pod CREATE**  
(문제 선택지라면 `c) Pod with CREATE operations`)

## 13. Webhook 동작 시나리오

1) **securityContext 미제공 + root 실행 위험 → 거부**  
2) **runAsNonRoot 미설정이면 기본값 자동 주입**
   - `runAsNonRoot: true`
   - `runAsUser: 1234`
3) **명시적으로 root 허용하고 싶다면**
   - `runAsNonRoot: false`를 직접 지정해야 허용

검증 방법:
- 생성 성공/실패 메시지 확인
- 생성 성공한 경우 `kubectl get pod <name> -o yaml`로 securityContext가 주입됐는지 확인

## 14. 프로세스에서 admission 플러그인 확인(왜 ps로 보나)
kube-apiserver가 Pod(컨테이너)로 실행되면,
노드에서 실행 중인 커맨드라인에 enable/disable 플래그가 보인다:
```bash
ps -ef | grep kube-apiserver | grep admission-plugins
```

## 15. 체크리스트

- 컨트롤플레인 Pod 확인: `kubectl get pods -n kube-system`
- 스태틱 파드 경로: `/var/lib/kubelet/config.yaml` → `staticPodPath`
- admission 플래그 변경: `/etc/kubernetes/manifests/kube-apiserver.yaml`
- 스케줄러 지정: Pod에 `spec.schedulerName`(오타 금지)
- webhook 영향 범위: `MutatingWebhookConfiguration.rules` 먼저 읽기
- webhook TLS: Secret(cert/key) + caBundle(검증) + Service(접근) 세트로 이해하기